{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import numpy as np\n",
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    #Construcutor\n",
    "    def __init__(self,neurons):\n",
    "        self.layers = len(neurons)\n",
    "        self.neurons = neurons\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        #Initialize weights\n",
    "        for i in range(1,self.layers):\n",
    "            #layer_weight = None\n",
    "            rows  = neurons[i]\n",
    "            cols = neurons[i-1]\n",
    "            #Creates a numpy array with dimensions rows x cols\n",
    "            #At the same time, initializes them with random normal distribution b/w 0 and 1\n",
    "            layer_weight = np.zeros((rows,cols))\n",
    "            self.weights.append(layer_weight)\n",
    "                \n",
    "            layer_bias = np.zeros((rows,1))\n",
    "            self.biases.append(layer_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(network,X):\n",
    "    \n",
    "    #Stores the Aj values\n",
    "    Layer_Activations = []\n",
    "    \n",
    "    #Stores the Zj values\n",
    "    Weighted_Sums = []\n",
    "    \n",
    "    for i in range(1,network.layers):\n",
    "   \n",
    "        if i==1:\n",
    "            A = X\n",
    "        else:\n",
    "            A  = Layer_Activations[-1]\n",
    "        \n",
    "        W = network.weights[i-1]\n",
    "        B = network.biases[i-1]\n",
    "        Z = np.dot(W,A) + B\n",
    "        Weighted_Sums.append(Z)\n",
    "        A = sigmoid(Z)\n",
    "        Layer_Activations.append(A)\n",
    "\n",
    "    return (Weighted_Sums,Layer_Activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Output,Target):\n",
    "    #Output --> (10,Number_of_training_Samples)\n",
    "    #Target --> (Number_of_traing_Samples)\n",
    "    \n",
    "    Prediction = np.argmax(Output,axis=0)\n",
    "    #Prediction --> (Number_of_training_Samples,)    \n",
    "    \n",
    "    accuracy = (sum((Prediction == Target).astype(np.float32))/Target.shape[0])*100.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    #print(\"sigmoid_derivative =\",sigmoid(z)*(1-sigmoid(z)) )\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_gradient(output,label):\n",
    "    #print(\"cost_gradient =\",output-label)\n",
    "    return output-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(network,delta_output_layer,weighted_sums):\n",
    "    #Initialize error for all layers\n",
    "    #delta is the backpropogation error\n",
    "    \n",
    "    #Number of output layers --> first layer is an input layer, hence excluded\n",
    "    n_out = network.layers-1\n",
    "    \n",
    "    delta_all_layers = [None]*(n_out)\n",
    "    \n",
    "    delta_all_layers[-1] = delta_output_layer\n",
    "    \n",
    "    for i in range(n_out-2,-1,-1):\n",
    "        delta_all_layers[i] = np.dot(network.weights[i+1].transpose(),delta_all_layers[i+1])*sigmoid_derivative(weighted_sums[i])\n",
    "            \n",
    "    return delta_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(network,X,layer_activations,delta_all_layers,alpha):\n",
    "    \n",
    "    n_weights = network.layers-1\n",
    "    for i in range(n_weights):\n",
    "        \n",
    "        print(\"shape of delta_all_layers[\",i,\"]:\",delta_all_layers[i].shape)\n",
    "        print(\"shape of layer_activations[\",i,\"].transpose():\",delta_all_layers[i].transpose().shape)            \n",
    "        if i==0:\n",
    "            layer_input = X\n",
    "        else:\n",
    "            layer_input = layer_activations[i-1]\n",
    "        \n",
    "        dcdw = np.dot(delta_all_layers[i],layer_input.transpose())\n",
    "        dcb = np.average(delta_all_layers[i],axis=1).reshape(delta_all_layers[i].shape[0],1)\n",
    "        print(\"shape of dcdw:\",dcdw.shape)\n",
    "        print(\"shape of dcb:\",dcb.shape)\n",
    "        print(\"shape of network.weights[\",i,\"]:\",network.weights[i].shape)\n",
    "        \n",
    "        network.weights[i] -= alpha * dcdw\n",
    "        network.biases[i] -= alpha * dcb\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of delta_all_layers[ 0 ]: (16, 50000)\n",
      "shape of layer_activations[ 0 ].transpose(): (50000, 16)\n",
      "shape of dcdw: (16, 784)\n",
      "shape of dcb: (16, 1)\n",
      "shape of network.weights[ 0 ]: (16, 784)\n",
      "shape of delta_all_layers[ 1 ]: (10, 50000)\n",
      "shape of layer_activations[ 1 ].transpose(): (50000, 10)\n",
      "shape of dcdw: (10, 16)\n",
      "shape of dcb: (10, 1)\n",
      "shape of network.weights[ 1 ]: (10, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2508.5  , -2508.5  , -2508.5  , -2508.5  , -2508.5  , -2508.5  ,\n",
       "        -2508.5  , -2508.5  , -2508.5  , -2508.5  , -2508.5  , -2508.5  ,\n",
       "        -2508.5  , -2508.5  , -2508.5  , -2508.5  ],\n",
       "       [-2415.25 , -2415.25 , -2415.25 , -2415.25 , -2415.25 , -2415.25 ,\n",
       "        -2415.25 , -2415.25 , -2415.25 , -2415.25 , -2415.25 , -2415.25 ,\n",
       "        -2415.25 , -2415.25 , -2415.25 , -2415.25 ],\n",
       "       [-2504.   , -2504.   , -2504.   , -2504.   , -2504.   , -2504.   ,\n",
       "        -2504.   , -2504.   , -2504.   , -2504.   , -2504.   , -2504.   ,\n",
       "        -2504.   , -2504.   , -2504.   , -2504.   ],\n",
       "       [-2487.375, -2487.375, -2487.375, -2487.375, -2487.375, -2487.375,\n",
       "        -2487.375, -2487.375, -2487.375, -2487.375, -2487.375, -2487.375,\n",
       "        -2487.375, -2487.375, -2487.375, -2487.375],\n",
       "       [-2517.625, -2517.625, -2517.625, -2517.625, -2517.625, -2517.625,\n",
       "        -2517.625, -2517.625, -2517.625, -2517.625, -2517.625, -2517.625,\n",
       "        -2517.625, -2517.625, -2517.625, -2517.625],\n",
       "       [-2561.75 , -2561.75 , -2561.75 , -2561.75 , -2561.75 , -2561.75 ,\n",
       "        -2561.75 , -2561.75 , -2561.75 , -2561.75 , -2561.75 , -2561.75 ,\n",
       "        -2561.75 , -2561.75 , -2561.75 , -2561.75 ],\n",
       "       [-2506.125, -2506.125, -2506.125, -2506.125, -2506.125, -2506.125,\n",
       "        -2506.125, -2506.125, -2506.125, -2506.125, -2506.125, -2506.125,\n",
       "        -2506.125, -2506.125, -2506.125, -2506.125],\n",
       "       [-2478.125, -2478.125, -2478.125, -2478.125, -2478.125, -2478.125,\n",
       "        -2478.125, -2478.125, -2478.125, -2478.125, -2478.125, -2478.125,\n",
       "        -2478.125, -2478.125, -2478.125, -2478.125],\n",
       "       [-2519.75 , -2519.75 , -2519.75 , -2519.75 , -2519.75 , -2519.75 ,\n",
       "        -2519.75 , -2519.75 , -2519.75 , -2519.75 , -2519.75 , -2519.75 ,\n",
       "        -2519.75 , -2519.75 , -2519.75 , -2519.75 ],\n",
       "       [-2501.5  , -2501.5  , -2501.5  , -2501.5  , -2501.5  , -2501.5  ,\n",
       "        -2501.5  , -2501.5  , -2501.5  , -2501.5  , -2501.5  , -2501.5  ,\n",
       "        -2501.5  , -2501.5  , -2501.5  , -2501.5  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data,valid_data,test_data = mnist_loader.load_data_wrapper()\n",
    "network = Network([784,16,10])\n",
    "weighted_sums,activations = feedforward(network,train_data[0])\n",
    "# print(activations[-1])\n",
    "delta_output_layer = cost_gradient(activations[-1],train_data[1])*sigmoid_derivative(weighted_sums[-1])\n",
    "delta_all_layers = backprop(network,delta_output_layer,weighted_sums)\n",
    "# delta_all_layers[::-1]\n",
    "update_weights(network,train_data[0],activations,delta_all_layers,1)\n",
    "network.weights[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
