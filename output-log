Dimensions of X,W,B: (50000, 784) (784, 28) (28,)
Dimensions of X,W,B: (50000, 28) (28, 10) (10,)
---Shape of last Weighted Sums layer =  (50000, 10)
+++Shape of do,sig_grad (50000, 10) (50000, 10)
---Shape of Error in output_layer =  (50000, 10)
Epoch, 0 : Error = 0.899998491487
network.weights.len =  2
***Backprop Error for layer : 0
Dimensions of weights[i+1],error[i+1] (28, 10) (50000, 10)
Dimensions of something,weighted_sums[i] (50000, 28) (50000, 28)
something =  [[-6.60350104 -3.88958566 -3.42117929 ..., -5.36560778 -4.70263091
  -6.69090411]
 [-6.0441316  -4.37314779 -4.17350633 ..., -5.60263566 -5.13230051
  -6.22821767]
 [-6.01144434 -3.87655174 -3.85205079 ..., -5.23586211 -4.79801086
  -6.4943133 ]
 ..., 
 [-6.37356968 -3.62670643 -4.09688329 ..., -5.42674678 -4.46034764
  -6.22235104]
 [-6.01144434 -3.87655174 -3.85205079 ..., -5.23586211 -4.79801086
  -6.4943133 ]
 [-6.37356968 -3.62670643 -4.09688329 ..., -5.42674678 -4.46034764
  -6.22235104]]
Weighted_Sums[ 0 ] = [[ 53.6227219   53.08729494  49.70226879 ...,  53.03731178  53.28725591
   59.66524426]
 [ 59.95929486  58.15528129  60.14767555 ...,  61.25307849  58.77340259
   64.79057855]
 [ 40.18255742  36.86480174  38.65381069 ...,  40.96995013  37.06431719
   33.00843059]
 ..., 
 [ 44.03212605  46.60246538  44.14603391 ...,  47.76096901  41.27968121
   46.33650438]
 [ 40.58588319  46.09615803  41.33104535 ...,  46.1819413   44.16133691
   42.57162267]
 [ 52.50690937  55.70763016  50.92798133 ...,  52.84622015  52.61725355
   52.11131958]]
sigmoid_gradient(Weighted_Sums[ 0 ]) = [[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   4.66293670e-15]
 ..., 
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]]
Dimensions of biases,db (28,) (28,)
Dimensions of biases,db (10,) (10,)
Dimensions of X,W,B: (50000, 784) (784, 28) (28,)
Dimensions of X,W,B: (50000, 28) (28, 10) (10,)
---Shape of last Weighted Sums layer =  (50000, 10)
+++Shape of do,sig_grad (50000, 10) (50000, 10)
---Shape of Error in output_layer =  (50000, 10)
Epoch, 1 : Error = -0.1
network.weights.len =  2
***Backprop Error for layer : 0
Dimensions of weights[i+1],error[i+1] (28, 10) (50000, 10)
Dimensions of something,weighted_sums[i] (50000, 28) (50000, 28)
something =  [[ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 ..., 
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]]
Weighted_Sums[ 0 ] = [[ 53.6227219   53.08729494  49.70226879 ...,  53.03731178  53.28725591
   59.66524426]
 [ 59.95929486  58.15528129  60.14767555 ...,  61.25307849  58.77340259
   64.79057855]
 [ 40.18255742  36.86480174  38.65381069 ...,  40.96995013  37.06431719
   33.00843059]
 ..., 
 [ 44.03212605  46.60246538  44.14603391 ...,  47.76096901  41.27968121
   46.33650438]
 [ 40.58588319  46.09615803  41.33104535 ...,  46.1819413   44.16133691
   42.57162267]
 [ 52.50690937  55.70763016  50.92798133 ...,  52.84622015  52.61725355
   52.11131958]]
/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp
  
/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide
  
/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide
  
/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in multiply
sigmoid_gradient(Weighted_Sums[ 0 ]) = [[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   4.66293670e-15]
 ..., 
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]]
Dimensions of biases,db (28,) (28,)
Dimensions of biases,db (10,) (10,)
Dimensions of X,W,B: (50000, 784) (784, 28) (28,)
Dimensions of X,W,B: (50000, 28) (28, 10) (10,)
---Shape of last Weighted Sums layer =  (50000, 10)
+++Shape of do,sig_grad (50000, 10) (50000, 10)
---Shape of Error in output_layer =  (50000, 10)
Epoch, 2 : Error = nan
network.weights.len =  2
***Backprop Error for layer : 0
Dimensions of weights[i+1],error[i+1] (28, 10) (50000, 10)
Dimensions of something,weighted_sums[i] (50000, 28) (50000, 28)
something =  [[ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 ..., 
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]]
 Weighted_Sums[ 0 ] = [[ 53.6227219   53.08729494  49.70226879 ...,  53.03731178  53.28725591
   59.66524426]
 [ 59.95929486  58.15528129  60.14767555 ...,  61.25307849  58.77340259
   64.79057855]
 [ 40.18255742  36.86480174  38.65381069 ...,  40.96995013  37.06431719
   33.00843059]
 ..., 
 [ 44.03212605  46.60246538  44.14603391 ...,  47.76096901  41.27968121
   46.33650438]
 [ 40.58588319  46.09615803  41.33104535 ...,  46.1819413   44.16133691
   42.57162267]
 [ 52.50690937  55.70763016  50.92798133 ...,  52.84622015  52.61725355
   52.11131958]]
sigmoid_gradient(Weighted_Sums[ 0 ]) = [[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   4.66293670e-15]
 ..., 
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00
    0.00000000e+00   0.00000000e+00]]