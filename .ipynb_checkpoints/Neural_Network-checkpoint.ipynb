{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import mnist_loader\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(X,W,B):\n",
    "    #y = wtranspose.x + biases\n",
    "    #print(\"Dimensions of X,W,B:\",X.shape,W.shape,B.shape)\n",
    "    Y = X.dot(W) + B\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X,network,Weighted_Sums,Inputs):\n",
    "    W = network.weights[0]\n",
    "    B = network.biases[0]\n",
    "    Inputs.append(X)\n",
    "    Y = weighted_sum(X,W,B)\n",
    "    Weighted_Sums.append(Y)\n",
    "    O = sigmoid(Y)\n",
    "    for i in range(1,len(network.weights)):\n",
    "        W = network.weights[i]\n",
    "        B = network.biases[i]\n",
    "        Inputs.append(O)\n",
    "        #Calculate weighted sum, wTx+B\n",
    "        Y = weighted_sum(O,W,B)\n",
    "        Weighted_Sums.append(Y)\n",
    "        O = sigmoid(Y)\n",
    "    #Final Output = maximum value from output classes (10 in the digit recognizer case)\n",
    "    #print(O)\n",
    "    #O = np.argmax(O,axis=1)+1\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogate the error from output layer..\n",
    "def backprop(network,error_output_layer,Weighted_Sums):\n",
    "\n",
    "    #Create a list of length --> network.weights and initialize it with None\n",
    "    error = [None]*len(network.weights)\n",
    "    error[-1] = error_output_layer\n",
    "    \n",
    "    #Goes from 2nd last layer to 2nd layer\n",
    "    \n",
    "    #4layers\n",
    "    #error[3] --> output\n",
    "    #len(nw.wets) --> 3\n",
    "    #3-1 --> 2\n",
    "    #for i in range(2,0) --> i ka values --> [2,1]\n",
    "    #print(\"network.weights.len = \",len(network.weights))\n",
    "    \n",
    "    #print(\"some_str\",some_variable,some_other,var,\"end_str\")\n",
    "    #print(\"abc\"+str(some_var))\n",
    "    \n",
    "    #len -> 2, then i = 0\n",
    "    for i in range(len(network.weights)-2,-1,-1):\n",
    "        #print(\"***Backprop Error for layer :\",i)\n",
    "        \n",
    "        #Backprop error\n",
    "        #δl=((wl+1)Tδl+1)⊙σ′(zl),\n",
    "        \n",
    "        #1st iteration ---> i=2nd last, i+1=last/output\n",
    "        #print(\"Dimensions of weights[i+1],error[i+1]\",network.weights[i+1].shape,error[i+1].shape)\n",
    "    \n",
    "        #print(\"error[\",i+1,\"] =\",error[i+1])\n",
    "        #print(\"np.transpose(network.weights[\",i+1,\"]) =\",np.transpose(network.weights[i+1]))\n",
    "        #Dimensions of weights[i+1],error[i+1] (28, 10) (5, 10)\n",
    "        something = error[i+1].dot(np.transpose(network.weights[i+1])) \n",
    "        #something---> should be (5,28)\n",
    "        \n",
    "        \n",
    "        #print(\"Dimensions of something,weighted_sums[i]\",something.shape,Weighted_Sums[i].shape)\n",
    "        # Dimensions of something,weighted_sums[i] (28,) (5, 28)\n",
    "        \n",
    "        \n",
    "        #print(\"something = \",something)\n",
    "        #print(\"Weighted_Sums[\",i,\"] =\",Weighted_Sums[i])\n",
    "        error[i] = something * sigmoid_gradient(Weighted_Sums[i]) \n",
    "        #print(\"sigmoid_gradient(Weighted_Sums[\",i,\"]) =\",sigmoid_gradient(Weighted_Sums[i]))\n",
    "        \n",
    "        #error[0]--> (,)\n",
    "        # (x,y) hadamard (5,28) --> (5,28)\n",
    "        \n",
    "    #print(\"Error = \",error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(network,Inputs,error,alpha):\n",
    "    \n",
    "    for i in range(len(network.weights)):\n",
    "\n",
    "        #Inputs contains an extra element X at the begining, hence i\n",
    "        #print(i,error[i]) #-->0,None, error[0]--None\n",
    "        \n",
    "        # dw = al-1 . errorl\n",
    "        dw = np.transpose(Inputs[i]).dot(error[i])\n",
    "\n",
    "        #Inputs[i] --> (5,784)\n",
    "        #error[i] --> (5,28)\n",
    "        # dw --> (784,28)\n",
    "        #Inputs --> list of all inputs. Inputs[0]--> X Inputs[1] --> O1\n",
    "        #Incoming Input --> to a Computational Neuron with some weights to that input\n",
    "\n",
    "        #error[i] --> for 5 training samples, you are getting change in biases\n",
    "        db = np.average(error[i],axis=0)\n",
    "        \n",
    "        \"\"\"\n",
    "            [\n",
    "                [1,2,3,4,5],\n",
    "                [2,3,5,6,7]\n",
    "            \n",
    "            ]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        network.weights[i] += alpha * dw\n",
    "        \n",
    "        #print(\"Dimensions of biases,db\",network.biases[i].shape,db.shape)\n",
    "        #Dimensions of biases,db (28,) (5, 28)\n",
    "        network.biases[i]  += alpha * db\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,X,t,mini_batch_size=10,alpha=0.1,epochs=100):\n",
    "    Inputs = []\n",
    "    Weighted_Sums = []\n",
    "    for i in range(epochs):     \n",
    "        \"\"\"\n",
    "        \n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        train_set_size = 50000\n",
    "        for k1 in range(0,train_set_size,mini_batch_size):\n",
    "            Xbatch = X[k1:k1+mini_batch_size]\n",
    "            tbatch = t[k1:k1+mini_batch_size]\n",
    "            \n",
    "            #Calculate output, while storing the weighted_sums and input at each layer\n",
    "            O = forward_pass(Xbatch,network,Weighted_Sums,Inputs)\n",
    "\n",
    "            #Calculate error in output layer\n",
    "            error_output_layer = calc_error(Weighted_Sums,O,tbatch)\n",
    "\n",
    "            #display_error = np.average(error_output_layer)\n",
    "            #print(display_error)\n",
    "\n",
    "            #print(\"Epoch\",i,\": Error = \"+str(display_error))\n",
    "\n",
    "            #Update weights\n",
    "            error = backprop(network,error_output_layer,Weighted_Sums)\n",
    "\n",
    "            update_weights(network,Inputs,error,alpha)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network,X,t):\n",
    "    Inputs = []\n",
    "    Weighted_Sums = []\n",
    "    O = forward_pass(X,network,Weighted_Sums,Inputs)\n",
    "    \n",
    "    O = np.argmax(O,axis=1)+1\n",
    "    display_error = np.average(np.where(O==t, 0, 1))\n",
    "    \n",
    "    print(\"Test_Error =\",display_error*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the error in output layer...\n",
    "def calc_error(Weighted_Sums,O,t):\n",
    "    \n",
    "    #partial-derivative -->dL/d(o) --> do\n",
    "    \n",
    "    #do = -(t/O + (1-t)/(1-O))\n",
    "    \n",
    "    do = O - t\n",
    "    #t --> (5,)\n",
    "    #O --> (5,)\n",
    "    #do --> (5,)\n",
    "    \n",
    "    #sigma-dash(ZsubL) --> sig_grad\n",
    "    sig_grad = sigmoid_gradient(Weighted_Sums[-1])\n",
    "    #--->\n",
    "    #print(\"---Shape of last Weighted Sums layer = \",Weighted_Sums[-1].shape)\n",
    "    \n",
    "    #numpy array consisting of errors for each neuron in the output layer\n",
    "    #print(\"+++Shape of do,sig_grad\",do.shape,sig_grad.shape)\n",
    "    #[1,2,3,4,5] dot [[1,0,0,0,],[],...[]]\n",
    "    #print(\"sig_grad = \",sig_grad)\n",
    "    #print(\"do = \",do)\n",
    "    \n",
    "    error_in_output_layer = do * sig_grad\n",
    "    \n",
    "    #(5,10) hadamard (5,10) --> (5,10)\n",
    "    \n",
    "    #should return np array with dims (5,10)\n",
    "    #print(\"---Shape of Error in output_layer = \",error_in_output_layer.shape)\n",
    "    #print(\"Error_in_output_layer =\",error_in_output_layer)\n",
    "    return error_in_output_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Function\n",
    "def ReLU(z):\n",
    "    return np.maximum(z,0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,layers):\n",
    "        if len(layers)<2:\n",
    "            print(\"Cannot create Neural Network with less than 2 layers\")\n",
    "            return\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(1, len(layers)):\n",
    "            rows = layers[i-1] #784\n",
    "            cols = layers[i] #28\n",
    "            layer_weight = np.random.random((rows,cols))\n",
    "            layer_bias = np.random.random(layers[i])\n",
    "            self.weights.append(layer_weight)\n",
    "            self.biases.append(layer_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = Network([784,28,10])\n",
    "#my_net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.random.random((5,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = np.array([\n",
    "#               [0,1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0,0],\n",
    "#               [0,1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0,0],\n",
    "#               [0,1,0,0,0,0,0,0,0,0]\n",
    "#             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "X = training_data[0]\n",
    "t = training_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,10) (50000,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2313868fd24f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c1df0b8cc8f4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network, X, t, mini_batch_size, alpha, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#Calculate error in output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0merror_output_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWeighted_Sums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#display_error = np.average(error_output_layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5502cfab4c8a>\u001b[0m in \u001b[0;36mcalc_error\u001b[0;34m(Weighted_Sums, O, t)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#do = -(t/O + (1-t)/(1-O))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#t --> (5,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#O --> (5,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,10) (50000,10) "
     ]
    }
   ],
   "source": [
    "train(my_net,X,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(my_net,test_data[0],test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(my_net,validation_data[0],validation_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0][:10].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
