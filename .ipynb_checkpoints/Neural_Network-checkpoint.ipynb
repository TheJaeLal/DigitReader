{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import mnist_loader\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(X,W,B):\n",
    "    #y = wtranspose.x + biases\n",
    "    print(\"Dimensions of X,W,B:\",X.shape,W.shape,B.shape)\n",
    "    Y = X.dot(W) + B\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X,network,Weighted_Sums,Inputs):\n",
    "    W = network.weights[0]\n",
    "    B = network.biases[0]\n",
    "    Inputs.append(X)\n",
    "    Y = weighted_sum(X,W,B)\n",
    "    Weighted_Sums.append(Y)\n",
    "    O = sigmoid(Y)\n",
    "    for i in range(1,len(network.weights)):\n",
    "        W = network.weights[i]\n",
    "        B = network.biases[i]\n",
    "        Inputs.append(O)\n",
    "        #Calculate weighted sum, wTx+B\n",
    "        Y = weighted_sum(O,W,B)\n",
    "        Weighted_Sums.append(Y)\n",
    "        O = sigmoid(Y)\n",
    "    #Final Output = maximum value from output classes (10 in the digit recognizer case)\n",
    "    #print(O)\n",
    "    #O = np.argmax(O,axis=1)+1\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogate the error from output layer..\n",
    "def backprop(network,error_output_layer,Weighted_Sums):\n",
    "\n",
    "    #Create a list of length --> network.weights and initialize it with None\n",
    "    error = [None]*len(network.weights)\n",
    "    error[-1] = error_output_layer\n",
    "    \n",
    "    #Goes from 2nd last layer to 2nd layer\n",
    "    \n",
    "    #4layers\n",
    "    #error[3] --> output\n",
    "    #len(nw.wets) --> 3\n",
    "    #3-1 --> 2\n",
    "    #for i in range(2,0) --> i ka values --> [2,1]\n",
    "    print(\"network.weights.len = \",len(network.weights))\n",
    "    \n",
    "    #print(\"some_str\",some_variable,some_other,var,\"end_str\")\n",
    "    #print(\"abc\"+str(some_var))\n",
    "    \n",
    "    #len -> 2, then i = [1]\n",
    "    for i in range(len(network.weights)-2,-1,-1):\n",
    "        print(\"***Backprop Error for layer :\",i)\n",
    "        \n",
    "        #Backprop error\n",
    "        #δl=((wl+1)Tδl+1)⊙σ′(zl),\n",
    "        \n",
    "        #1st iteration ---> i=2nd last, i+1=last/output\n",
    "        print(\"Dimensions of weights[i+1],error[i+1]\",network.weights[i+1].shape,error[i+1].shape)\n",
    "    \n",
    "\n",
    "        #Dimensions of weights[i+1],error[i+1] (28, 10) (5, 10)\n",
    "        something = error[i+1].dot(np.transpose(network.weights[i+1])) \n",
    "        #something---> should be (5,28)\n",
    "        \n",
    "        \n",
    "        print(\"Dimensions of something,weighted_sums[i]\",something.shape,Weighted_Sums[i].shape)\n",
    "        # Dimensions of something,weighted_sums[i] (28,) (5, 28)\n",
    "        \n",
    "        \n",
    "        error[i] = something * sigmoid_gradient(Weighted_Sums[i]) \n",
    "        \n",
    "        #error[0]--> (,)\n",
    "        # (x,y) hadamard (5,28) --> (5,28)\n",
    "        \n",
    "    #print(\"Error = \",error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(network,Inputs,error,alpha):\n",
    "    \n",
    "    for i in range(len(network.weights)):\n",
    "\n",
    "        #Inputs contains an extra element X at the begining, hence i\n",
    "        #print(i,error[i]) #-->0,None, error[0]--None\n",
    "        \n",
    "        # dw = al-1 . errorl\n",
    "        dw = np.transpose(Inputs[i]).dot(error[i])\n",
    "\n",
    "        #Inputs[i] --> (5,784)\n",
    "        #error[i] --> (5,28)\n",
    "        # dw --> (784,28)\n",
    "        #Inputs --> list of all inputs. Inputs[0]--> X Inputs[1] --> O1\n",
    "        #Incoming Input --> to a Computational Neuron with some weights to that input\n",
    "\n",
    "        #error[i] --> for 5 training samples, you are getting change in biases\n",
    "        db = np.average(error[i],axis=0)\n",
    "        \"\"\"\n",
    "            [\n",
    "                [1,2,3,4,5],\n",
    "                [2,3,5,6,7]\n",
    "            \n",
    "            ]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        network.weights[i] += alpha * dw\n",
    "        \n",
    "        print(\"Dimensions of biases,db\",network.biases[i].shape,db.shape)\n",
    "        #Dimensions of biases,db (28,) (5, 28)\n",
    "        network.biases[i]  += alpha * db\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,X,t,alpha=0.25,epochs=100):\n",
    "    Inputs = []\n",
    "    Weighted_Sums = []\n",
    "    for i in range(epochs):\n",
    "        #Calculate output, while storing the weighted_sums and input at each layer\n",
    "        O = forward_pass(X,network,Weighted_Sums,Inputs)\n",
    "        \n",
    "        #Calculate error in output layer\n",
    "        error_output_layer = calc_error(Weighted_Sums,O,t)\n",
    "        \n",
    "        display_error = np.average(O-t)\n",
    "        #print(display_error)\n",
    "        \n",
    "        print(\"Epoch,\",i,\": Error = \"+str(display_error))\n",
    "        \n",
    "        #Update weights\n",
    "        error = backprop(network,error_output_layer,Weighted_Sums)\n",
    "        \n",
    "        update_weights(network,Inputs,error,alpha)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the error in output layer...\n",
    "def calc_error(Weighted_Sums,O,t):\n",
    "    \n",
    "    #partial-derivative -->dL/d(o) --> do\n",
    "    \n",
    "    do = -(t/O + (1-t)/(1-O))\n",
    "    \n",
    "    #t --> (5,)\n",
    "    #O --> (5,)\n",
    "    #do --> (5,)\n",
    "    \n",
    "    #sigma-dash(ZsubL) --> sig_grad\n",
    "    sig_grad = sigmoid_gradient(Weighted_Sums[-1])\n",
    "    #--->\n",
    "    print(\"---Shape of last Weighted Sums layer = \",Weighted_Sums[-1].shape)\n",
    "    \n",
    "    #numpy array consisting of errors for each neuron in the output layer\n",
    "    print(\"+++Shape of do,sig_grad\",do.shape,sig_grad.shape)\n",
    "    #[1,2,3,4,5] dot [[1,0,0,0,],[],...[]]\n",
    "    #print(\"sig_grad = \",sig_grad)\n",
    "    #print(\"do = \",do)\n",
    "    error_in_output_layer = do * sig_grad\n",
    "    #(5,10) hadamard (5,10) --> (5,10)\n",
    "    \n",
    "    #should return np array with dims (5,10)\n",
    "    print(\"---Shape of Error in output_layer = \",error_in_output_layer.shape)\n",
    "    return error_in_output_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return np.exp(-z)/((1.0 + np.exp(-z))**2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Function\n",
    "def ReLU(z):\n",
    "    return np.maximum(z,0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(np.exp(-z) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,layers):\n",
    "        if len(layers)<2:\n",
    "            print(\"Cannot create Neural Network with less than 2 layers\")\n",
    "            return\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(1, len(layers)):\n",
    "            rows = layers[i-1] #784\n",
    "            cols = layers[i] #28\n",
    "            layer_weight = np.random.random((rows,cols))\n",
    "            layer_bias = np.random.random(layers[i])\n",
    "            self.weights.append(layer_weight)\n",
    "            self.biases.append(layer_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = Network([784,28,10])\n",
    "#my_net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.random.random((5,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = np.array([\n",
    "#               [0,1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0,0],\n",
    "#               [0,1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0,0],\n",
    "#               [0,1,0,0,0,0,0,0,0,0]\n",
    "#             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "X = training_data[0]\n",
    "t = training_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(my_net,X,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
