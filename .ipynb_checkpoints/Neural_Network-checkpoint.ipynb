{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import numpy as np\n",
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    #Construcutor\n",
    "    def __init__(self,neurons):\n",
    "        self.layers = len(neurons)\n",
    "        self.neurons = neurons\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        #Initialize weights\n",
    "        for i in range(1,self.layers):\n",
    "            #layer_weight = None\n",
    "            rows  = neurons[i]\n",
    "            cols = neurons[i-1]\n",
    "            #Creates a numpy array with dimensions rows x cols\n",
    "            #At the same time, initializes them with random normal distribution b/w 0 and 1\n",
    "            layer_weight = np.zeros((rows,cols))\n",
    "            self.weights.append(layer_weight)\n",
    "                \n",
    "            layer_bias = np.zeros((rows,1))\n",
    "            self.biases.append(layer_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(network,X):\n",
    "    \n",
    "    #Stores the Aj values\n",
    "    Layer_Activations = []\n",
    "    \n",
    "    #Stores the Zj values\n",
    "    Weighted_Sums = []\n",
    "    \n",
    "    for i in range(1,network.layers):\n",
    "   \n",
    "        if i==1:\n",
    "            A = X\n",
    "        else:\n",
    "            A  = Layer_Activations[-1]\n",
    "        \n",
    "        W = network.weights[i-1]\n",
    "        B = network.biases[i-1]\n",
    "        Z = np.dot(W,A) + B\n",
    "        Weighted_Sums.append(Z)\n",
    "        A = sigmoid(Z)\n",
    "        Layer_Activations.append(A)\n",
    "\n",
    "    return (Weighted_Sums,Layer_Activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Output,Target):\n",
    "    #Output --> (10,Number_of_training_Samples)\n",
    "    #Target --> (Number_of_traing_Samples)\n",
    "    \n",
    "    Prediction = np.argmax(Output,axis=0)\n",
    "    #Prediction --> (Number_of_training_Samples,)    \n",
    "    \n",
    "    accuracy = (sum((Prediction == Target).astype(np.float32))/Target.shape[0])*100.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    #print(\"sigmoid_derivative =\",sigmoid(z)*(1-sigmoid(z)) )\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_gradient(output,label):\n",
    "    #print(\"cost_gradient =\",output-label)\n",
    "    return output-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(network,delta_output_layer,weighted_sums):\n",
    "    #Initialize error for all layers\n",
    "    #delta is the backpropogation error\n",
    "    \n",
    "    #Number of output layers --> first layer is an input layer, hence excluded\n",
    "    n_out = network.layers-1\n",
    "    \n",
    "    delta_all_layers = [None]*(n_out)\n",
    "    \n",
    "    delta_all_layers[-1] = delta_output_layer\n",
    "    \n",
    "    for i in range(n_out-2,-1,-1):\n",
    "        delta_all_layers[i] = np.dot(network.weights[i+1].transpose(),delta_all_layers[i+1])*sigmoid_derivative(weighted_sums[i])\n",
    "            \n",
    "    return delta_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([[ 0.125, -0.125,  0.125, ...,  0.125,  0.125,  0.125],\n",
       "        [ 0.125,  0.125,  0.125, ...,  0.125,  0.125,  0.125],\n",
       "        [ 0.125,  0.125,  0.125, ...,  0.125,  0.125,  0.125],\n",
       "        ..., \n",
       "        [ 0.125,  0.125,  0.125, ...,  0.125,  0.125,  0.125],\n",
       "        [ 0.125,  0.125,  0.125, ..., -0.125,  0.125, -0.125],\n",
       "        [ 0.125,  0.125,  0.125, ...,  0.125,  0.125,  0.125]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data,valid_data,test_data = mnist_loader.load_data_wrapper()\n",
    "network = Network([784,16,10])\n",
    "weighted_sums,activations = feedforward(network,train_data[0])\n",
    "# print(activations[-1])\n",
    "delta_output_layer = cost_gradient(activations[-1],train_data[1])*sigmoid_derivative(weighted_sums[-1])\n",
    "delta_all_layers = backprop(network,delta_output_layer,weighted_sums)\n",
    "delta_all_layers[::-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
